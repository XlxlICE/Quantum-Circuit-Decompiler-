{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the QASM Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\course\\thesis\\Project2\\QCD\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from algorthims import *\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QASM_generator(circuitname, max_qubit):\n",
    "    # 创建目录\n",
    "    directory = \"Circuits\"\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    \n",
    "    circuit_func = None\n",
    "    if circuitname == \"grover\":\n",
    "        circuit_func = grover\n",
    "    elif circuitname == \"qft\":\n",
    "        circuit_func = qft\n",
    "    elif circuitname == \"qpe\":\n",
    "        circuit_func = qpe\n",
    "    elif circuitname == \"h_c\":\n",
    "        circuit_func = h_c\n",
    "    elif circuitname == \"rx_c\":\n",
    "        circuit_func = rx_c\n",
    "    elif circuitname == \"rx_gradually_c\":\n",
    "        circuit_func = rx_gradually_c  \n",
    "    else:\n",
    "        print(\"Unsupported circuit name.\")\n",
    "        return\n",
    "\n",
    "    for n in range(2, max_qubit + 1):\n",
    "      \n",
    "        circuit = circuit_func(n)\n",
    "       \n",
    "        qasm_str = circuit.qasm()\n",
    "\n",
    "        filename = os.path.join(directory, f\"{circuitname}_{n}.qasm\")\n",
    "        with open(filename, \"w\") as file:\n",
    "            file.write(qasm_str)\n",
    "        print(f\"Saved {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QASM_generator('qft',20)\n",
    "# QASM_generator('qpe',20)\n",
    "# QASM_generator('grover',20)\n",
    "# QASM_generator('h_c',40)\n",
    "# QASM_generator('rx_c',40)\n",
    "# QASM_generator('rx_gradually_c',40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\programing\\Anaconda\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "d:\\programing\\Anaconda\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "d:\\programing\\Anaconda\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "# Relevant imports\n",
    "\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from tokenizers.pre_tokenizers import CharDelimiterSplit\n",
    "import json\n",
    "import os\n",
    "from functools import lru_cache\n",
    "from typing import TYPE_CHECKING, List, Optional, Tuple\n",
    "import regex as re\n",
    "from transformers.tokenization_utils import AddedToken, PreTrainedTokenizer\n",
    "from transformers.utils import logging\n",
    "if TYPE_CHECKING:\n",
    "    from transformers.pipelines.conversational import Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining my own GPT2Tokenizer class to circumvent BPE tokenisation\n",
    "# Most code is from the Huggingface implementation\n",
    "\n",
    "logger = logging.get_logger(__name__)\n",
    "\n",
    "VOCAB_FILES_NAMES = {\n",
    "    \"vocab_file\": \"vocab.json\",\n",
    "    \"merges_file\": \"merges.txt\",\n",
    "}\n",
    "\n",
    "PRETRAINED_VOCAB_FILES_MAP = {\n",
    "    \"vocab_file\": {\n",
    "        \"gpt2\": \"https://huggingface.co/gpt2/resolve/main/vocab.json\",\n",
    "        \"gpt2-medium\": \"https://huggingface.co/gpt2-medium/resolve/main/vocab.json\",\n",
    "        \"gpt2-large\": \"https://huggingface.co/gpt2-large/resolve/main/vocab.json\",\n",
    "        \"gpt2-xl\": \"https://huggingface.co/gpt2-xl/resolve/main/vocab.json\",\n",
    "        \"distilgpt2\": \"https://huggingface.co/distilgpt2/resolve/main/vocab.json\",\n",
    "    },\n",
    "    \"merges_file\": {\n",
    "        \"gpt2\": \"https://huggingface.co/gpt2/resolve/main/merges.txt\",\n",
    "        \"gpt2-medium\": \"https://huggingface.co/gpt2-medium/resolve/main/merges.txt\",\n",
    "        \"gpt2-large\": \"https://huggingface.co/gpt2-large/resolve/main/merges.txt\",\n",
    "        \"gpt2-xl\": \"https://huggingface.co/gpt2-xl/resolve/main/merges.txt\",\n",
    "        \"distilgpt2\": \"https://huggingface.co/distilgpt2/resolve/main/merges.txt\",\n",
    "    },\n",
    "}\n",
    "\n",
    "PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES = {\n",
    "    \"gpt2\": 1024,\n",
    "    \"gpt2-medium\": 1024,\n",
    "    \"gpt2-large\": 1024,\n",
    "    \"gpt2-xl\": 1024,\n",
    "    \"distilgpt2\": 1024,\n",
    "}\n",
    "\n",
    "class GPT2Tokenizer(PreTrainedTokenizer):\n",
    "    \"\"\"\n",
    "    Construct a GPT-2 tokenizer. Based on byte-level Byte-Pair-Encoding.\n",
    "    This tokenizer has been trained to treat spaces like parts of the tokens (a bit like sentencepiece) so a word will\n",
    "    be encoded differently whether it is at the beginning of the sentence (without space) or not:\n",
    "    ```\n",
    "    >>> from transformers import GPT2Tokenizer\n",
    "    >>> tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "    >>> tokenizer(\"Hello world\")['input_ids']\n",
    "    [15496, 995]\n",
    "    >>> tokenizer(\" Hello world\")['input_ids']\n",
    "    [18435, 995]\n",
    "    ```\n",
    "    You can get around that behavior by passing `add_prefix_space=True` when instantiating this tokenizer or when you\n",
    "    call it on some text, but since the model was not pretrained this way, it might yield a decrease in performance.\n",
    "    <Tip>\n",
    "    When used with `is_split_into_words=True`, this tokenizer will add a space before each word (even the first one).\n",
    "    </Tip>\n",
    "    This tokenizer inherits from [`PreTrainedTokenizer`] which contains most of the main methods. Users should refer to\n",
    "    this superclass for more information regarding those methods.\n",
    "    Args:\n",
    "        vocab_file (`str`):\n",
    "            Path to the vocabulary file.\n",
    "        merges_file (`str`):\n",
    "            Path to the merges file.\n",
    "        errors (`str`, *optional*, defaults to `\"replace\"`):\n",
    "            Paradigm to follow when decoding bytes to UTF-8. See\n",
    "            [bytes.decode](https://docs.python.org/3/library/stdtypes.html#bytes.decode) for more information.\n",
    "        unk_token (`str`, *optional*, defaults to `<|endoftext|>`):\n",
    "            The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this\n",
    "            token instead.\n",
    "        bos_token (`str`, *optional*, defaults to `<|endoftext|>`):\n",
    "            The beginning of sequence token.\n",
    "        eos_token (`str`, *optional*, defaults to `<|endoftext|>`):\n",
    "            The end of sequence token.\n",
    "        add_prefix_space (`bool`, *optional*, defaults to `False`):\n",
    "            Whether or not to add an initial space to the input. This allows to treat the leading word just as any\n",
    "            other word. (GPT2 tokenizer detect beginning of words by the preceding space).\n",
    "    \"\"\"\n",
    "\n",
    "    vocab_files_names = VOCAB_FILES_NAMES\n",
    "    max_model_input_sizes = PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES\n",
    "    model_input_names = [\"input_ids\", \"attention_mask\"]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_file,\n",
    "        merges_file,\n",
    "        errors=\"replace\",\n",
    "        unk_token=\"<|endoftext|>\",\n",
    "        bos_token=\"<|endoftext|>\",\n",
    "        eos_token=\"<|endoftext|>\",\n",
    "        pad_token=None,\n",
    "        add_prefix_space=False,\n",
    "        add_bos_token=False,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        bos_token = AddedToken(bos_token, lstrip=False, rstrip=False) if isinstance(bos_token, str) else bos_token\n",
    "        eos_token = AddedToken(eos_token, lstrip=False, rstrip=False) if isinstance(eos_token, str) else eos_token\n",
    "        unk_token = AddedToken(unk_token, lstrip=False, rstrip=False) if isinstance(unk_token, str) else unk_token\n",
    "        pad_token = AddedToken(pad_token, lstrip=False, rstrip=False) if isinstance(pad_token, str) else pad_token\n",
    "        super().__init__(\n",
    "            errors=errors,\n",
    "            unk_token=unk_token,\n",
    "            bos_token=bos_token,\n",
    "            eos_token=eos_token,\n",
    "            pad_token=pad_token,\n",
    "            add_prefix_space=add_prefix_space,\n",
    "            add_bos_token=add_bos_token,\n",
    "            **kwargs,\n",
    "        )\n",
    "        self.add_bos_token = add_bos_token\n",
    "\n",
    "        with open(vocab_file, encoding=\"utf-8\") as vocab_handle:\n",
    "            self.encoder = json.load(vocab_handle)\n",
    "        self.decoder = {v: k for k, v in self.encoder.items()}\n",
    "        self.errors = errors  # how to handle errors in decoding\n",
    "        self.byte_encoder = self.encoder\n",
    "        self.byte_decoder = self.decoder\n",
    "        with open(merges_file, encoding=\"utf-8\") as merges_handle:\n",
    "            bpe_merges = merges_handle.read().split(\"\\n\")[1:-1]\n",
    "        bpe_merges = [tuple(merge.split()) for merge in bpe_merges]\n",
    "        self.bpe_ranks = dict(zip(bpe_merges, range(len(bpe_merges))))\n",
    "        self.cache = {}\n",
    "        self.add_prefix_space = add_prefix_space\n",
    "\n",
    "        # Should have added re.IGNORECASE so BPE merges can happen for capitalized versions of contractions\n",
    "        #self.pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return len(self.encoder)\n",
    "\n",
    "    def get_vocab(self):\n",
    "        return dict(self.encoder, **self.added_tokens_encoder)\n",
    "\n",
    "    def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n",
    "        if self.add_bos_token:\n",
    "            bos_token_ids = [self.bos_token_id]\n",
    "        else:\n",
    "            bos_token_ids = []\n",
    "\n",
    "        output = bos_token_ids + token_ids_0\n",
    "\n",
    "        if token_ids_1 is None:\n",
    "            return output\n",
    "\n",
    "        return output + bos_token_ids + token_ids_1\n",
    "\n",
    " #   def get_special_tokens_mask(\n",
    " #       self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None, already_has_special_tokens: bool = False\n",
    " #   ) -> List[int]:\n",
    "        \"\"\"\n",
    "        Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding\n",
    "        special tokens using the tokenizer `prepare_for_model` or `encode_plus` methods.\n",
    "        Args:\n",
    "            token_ids_0 (`List[int]`):\n",
    "                List of IDs.\n",
    "            token_ids_1 (`List[int]`, *optional*):\n",
    "                Optional second list of IDs for sequence pairs.\n",
    "            already_has_special_tokens (`bool`, *optional*, defaults to `False`):\n",
    "                Whether or not the token list is already formatted with special tokens for the model.\n",
    "        Returns:\n",
    "            `List[int]`: A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.\n",
    "        \"\"\"\n",
    " #       if already_has_special_tokens:\n",
    " #           return super().get_special_tokens_mask(\n",
    " #               token_ids_0=token_ids_0, token_ids_1=token_ids_1, already_has_special_tokens=True\n",
    " #           )\n",
    "\n",
    "  #      if not self.add_bos_token:\n",
    "   #         return super().get_special_tokens_mask(\n",
    "    #            token_ids_0=token_ids_0, token_ids_1=token_ids_1, already_has_special_tokens=False\n",
    "     #       )\n",
    "\n",
    "      #  if token_ids_1 is None:\n",
    "       #     return [1] + ([0] * len(token_ids_0))\n",
    "        #return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1))\n",
    "\n",
    "    def tokenize_boran(self, text):\n",
    "        \"\"\"Tokenize a string.\"\"\"\n",
    "        #bpe_tokens = []\n",
    "        #for token in re.findall(self.pat, text): #This was the huggingface implementation\n",
    "        #    token = \"\".join(\n",
    "        #        self.byte_encoder[b] for b in token.encode(\"utf-8\")\n",
    "        #    )  # Maps all our bytes to unicode strings, avoiding control tokens of the BPE (spaces in our case)\n",
    "        #    bpe_tokens.extend(bpe_token for bpe_token in self.bpe(token).split(\" \"))\n",
    "        #bpe_token = self.encoder.get(text)\n",
    "        #bpe_tokens.extend([bpe_token])\n",
    "        bpe_tokens = text\n",
    "        try:\n",
    "          if text[0][0] == '[':\n",
    "            bpe_tokens = []\n",
    "            for subtext_index, subtext in enumerate(text):\n",
    "              text_replaced = text[subtext_index].replace(\"'\", \"\")\n",
    "              text_replaced = text_replaced.replace(\"\\\\n\", \"\\n\")\n",
    "              text_replaced = text_replaced.replace(\"\\\\t\", \"\\t\") #new\n",
    "              text_replaced = text_replaced[1:]\n",
    "              bpe_tokens.append(text_replaced[:-1].split(', '))\n",
    "\n",
    "        except:\n",
    "          bpe_tokens = text\n",
    "\n",
    "        return bpe_tokens\n",
    "    \n",
    "    def tokenize(self, text, is_split_into_words = True):\n",
    "      bpe_tokens = text\n",
    "      return text\n",
    "\n",
    "\n",
    "    def _convert_token_to_id(self, token):\n",
    "        \"\"\"Converts a token (str) in an id using the vocab.\"\"\"\n",
    "        return self.encoder.get(token, self.encoder.get(self.unk_token))\n",
    "\n",
    "    def _convert_id_to_token(self, index):\n",
    "        \"\"\"Converts an index (integer) in a token (str) using the vocab.\"\"\"\n",
    "        return self.decoder.get(index)\n",
    "\n",
    "    def convert_tokens_to_string(self, tokens):\n",
    "        \"\"\"Converts a sequence of tokens (string) in a single string.\"\"\"\n",
    "        text = \"\".join(tokens)\n",
    "        text = self.byte_decoder[text]\n",
    "        return text\n",
    "\n",
    "    def convert_tokens_to_string(self, tokens: List[str]) -> str:\n",
    "        return tokens\n",
    "\n",
    "    def convert_tokens_to_ids(self, tokens):\n",
    "      return self.encoder.get(tokens, self.encoder.get(self.unk_token))\n",
    "\n",
    "    def convert_tokens_to_ids(self, tokens):\n",
    "      try:\n",
    "        ids = [self.encoder.get(tokens)]\n",
    "        if tokens == '<pad>' or tokens == '<s>' or tokens == '</s>' or tokens == '<mask>':\n",
    "          ids = self.encoder.get(tokens)\n",
    "      except:\n",
    "        try:\n",
    "          ids = []\n",
    "          for token in tokens:\n",
    "            id = self.encoder.get(token)\n",
    "            ids.append(id)\n",
    "        except:\n",
    "          ids = []\n",
    "          ids_list = []\n",
    "          for lists in tokens:\n",
    "            for token in lists:\n",
    "              id = self.encoder.get(token)\n",
    "              ids.append(id)\n",
    "            ids_list.append(ids)\n",
    "          ids = ids_list\n",
    "      return ids\n",
    "\n",
    "    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]:\n",
    "        if not os.path.isdir(save_directory):\n",
    "            logger.error(f\"Vocabulary path ({save_directory}) should be a directory\")\n",
    "            return\n",
    "        vocab_file = os.path.join(\n",
    "            save_directory, (filename_prefix + \"-\" if filename_prefix else \"\") + VOCAB_FILES_NAMES[\"vocab_file\"]\n",
    "        )\n",
    "        merge_file = os.path.join(\n",
    "            save_directory, (filename_prefix + \"-\" if filename_prefix else \"\") + VOCAB_FILES_NAMES[\"merges_file\"]\n",
    "        )\n",
    "\n",
    "        with open(vocab_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(json.dumps(self.encoder, indent=2, sort_keys=True, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "        index = 0\n",
    "        with open(merge_file, \"w\", encoding=\"utf-8\") as writer:\n",
    "            writer.write(\"#version: 0.2\\n\")\n",
    "            for bpe_tokens, token_index in sorted(self.bpe_ranks.items(), key=lambda kv: kv[1]):\n",
    "                if index != token_index:\n",
    "                    logger.warning(\n",
    "                        f\"Saving vocabulary to {merge_file}: BPE merge indices are not consecutive.\"\n",
    "                        \" Please check that the tokenizer is not corrupted!\"\n",
    "                    )\n",
    "                    index = token_index\n",
    "                writer.write(\" \".join(bpe_tokens) + \"\\n\")\n",
    "                index += 1\n",
    "\n",
    "        return vocab_file, merge_file\n",
    "\n",
    "    def prepare_for_tokenization(self, text, is_split_into_words=False, **kwargs):\n",
    "        add_prefix_space = kwargs.pop(\"add_prefix_space\", self.add_prefix_space)\n",
    "        if is_split_into_words or add_prefix_space:\n",
    "            text = text\n",
    "        return (text, kwargs)\n",
    "\n",
    "    def _build_conversation_input_ids(self, conversation: \"Conversation\") -> List[int]:\n",
    "        input_ids = []\n",
    "        for is_user, text in conversation.iter_texts():\n",
    "            input_ids.extend(self.encode(text, add_special_tokens=False) + [self.eos_token_id])\n",
    "        if len(input_ids) > self.model_max_length:\n",
    "            input_ids = input_ids[-self.model_max_length :]\n",
    "        return input_ids\n",
    "    \n",
    "    def _batch_encode_plus(self, *args, **kwargs):\n",
    "      # Custom implementation of _batch_encode_plus\n",
    "      result = super()._batch_encode_plus(*args, **kwargs)\n",
    "      # Modify the output format\n",
    "      new_ids = result[\"input_ids\"]\n",
    "      new_masks = result[\"attention_mask\"]\n",
    "      result[\"input_ids\"] = [item for sublist in new_ids for item in sublist]\n",
    "      result[\"attention_mask\"] = [item for sublist in new_masks for item in sublist]\n",
    "      if 0 == True:\n",
    "        result[\"input_ids\"] = result[\"input_ids\"][:3]\n",
    "        result[\"attention_mask\"] = result[\"attention_mask\"][:3]\n",
    "\n",
    "      return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the tokenizer\n",
    "# Minor bug: Prints a number for some reason\n",
    "with open('gpt_tokenizer/vocab.json', encoding=\"utf-8\") as vocab_handle:\n",
    "        GPT2Tokenizer.encoder = json.load(vocab_handle)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt_tokenizer')\n",
    "tokenizer.add_special_tokens({\n",
    "    \"eos_token\": \"</s>\",\n",
    "    \"bos_token\": \"<s>\",\n",
    "    \"unk_token\": \"<unk>\",\n",
    "    \"pad_token\": \"<pad>\",\n",
    "    \"mask_token\": \"<mask>\"\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test to see if our tokenizer works\n",
    "\n",
    "tokenizer(['h q[0];\\n', 'h q[1];\\n'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular Expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantum Register Declarations:\n",
      "qreg q[7];\n",
      "\n",
      "Quantum Gate Operations:\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def extract_qasm_info(algorithm, qubit_count):\n",
    "    \"\"\"\n",
    "    Extracts quantum register declarations and quantum gate operations from a QASM file based on the algorithm name and number of qubits.\n",
    "\n",
    "    Parameters:\n",
    "    algorithm (str): The name of the algorithm.\n",
    "    qubit_count (int): The number of qubits.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Construct the file path using a raw string\n",
    "    file_path = fr\"D:\\course\\thesis\\Project2\\QCD\\Circuits\\{algorithm}_{qubit_count}.qasm\"\n",
    "\n",
    "    try:\n",
    "        # Read the content of the file\n",
    "        with open(file_path, 'r') as file:\n",
    "            qasm_content = file.read()\n",
    "\n",
    "        # Regular expressions to match quantum register declarations and quantum gate operations\n",
    "        qreg_pattern = re.compile(r\"qreg\\s+\\w+\\[\\d+\\];\")\n",
    "        gate_pattern = re.compile(r\"\\b(h|cx)\\b\\s+\\w+\\[\\d+\\](,\\w+\\[\\d+\\])?;\")\n",
    "\n",
    "        # Use the regular expressions to find all matches\n",
    "        qreg_matches = qreg_pattern.findall(qasm_content)\n",
    "        gate_matches = gate_pattern.findall(qasm_content)\n",
    "\n",
    "        # Output the results\n",
    "        print(\"Quantum Register Declarations:\")\n",
    "        for match in qreg_matches:\n",
    "            print(match)\n",
    "\n",
    "        print(\"\\nQuantum Gate Operations:\")\n",
    "        for match in gate_matches:\n",
    "            print(match[0])\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {algorithm}_{qubit_count}.qasm not found in the specified path.\")\n",
    "\n",
    "# Example usage\n",
    "extract_qasm_info(\"qft\", 7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOF while scanning triple-quoted string literal (2842501474.py, line 40)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[10], line 40\u001b[1;36m\u001b[0m\n\u001b[1;33m    gate_QFT q[0],q[1],q[2],q[3\u001b[0m\n\u001b[1;37m                               \n^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m EOF while scanning triple-quoted string literal\n"
     ]
    }
   ],
   "source": [
    "def extract_qasm_info(algorithm, qubit_count):\n",
    "    \"\"\"\n",
    "    Extracts quantum register declarations and quantum gate operations from a QASM file based on the algorithm name and number of qubits.\n",
    "\n",
    "    Parameters:\n",
    "    algorithm (str): The name of the algorithm.\n",
    "    qubit_count (int): The number of qubits.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Construct the file path using a raw string\n",
    "    file_path = fr\"D:\\course\\thesis\\Project2\\QCD\\Circuits\\{algorithm}_{qubit_count}.qasm\"\n",
    "# Regular expressions to match custom gate applications and h gate operations\n",
    "    custom_gate_pattern = re.compile(r\"\\bgate_QFT\\b\")\n",
    "    h_gate_pattern = re.compile(r\"\\bh\\s+\\w+\\[\\d+\\];\")\n",
    "\n",
    "    # Use the regular expressions to find all matches\n",
    "    custom_gate_matches = custom_gate_pattern.findall(qasm_content)\n",
    "    h_gate_matches = h_gate_pattern.findall(qasm_content)\n",
    "\n",
    "    # Output the results\n",
    "    print(\"Custom Gate Applications:\")\n",
    "    for match in custom_gate_matches:\n",
    "        print(match)\n",
    "\n",
    "    print(\"\\nHadamard Gate Operations within Custom Gates:\")\n",
    "    for match in h_gate_matches:\n",
    "        print(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\programing\\Anaconda\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "d:\\programing\\Anaconda\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "d:\\programing\\Anaconda\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        ┌───────────┐ ░ ┌─┐   \n",
      "   q_0: ┤0          ├─░─┤M├───\n",
      "        │  Gate_qft │ ░ └╥┘┌─┐\n",
      "   q_1: ┤1          ├─░──╫─┤M├\n",
      "        └───────────┘ ░  ║ └╥┘\n",
      "   c: 2/═════════════════╬══╬═\n",
      "                         ║  ║ \n",
      "meas: 2/═════════════════╩══╩═\n",
      "                         0  1 \n"
     ]
    }
   ],
   "source": [
    "from qiskit import QuantumCircuit\n",
    "\n",
    "# Path to your QASM file\n",
    "qasm_file_path = 'Circuits\\qft_2.qasm'\n",
    "# Read the QASM file\n",
    "quantum_circuit = QuantumCircuit.from_qasm_file(qasm_file_path)\n",
    "\n",
    "# Print the quantum circuit to see its contents\n",
    "print(quantum_circuit)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## genetic programming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from algorthims import *\n",
    "from tqdm import tqdm\n",
    "\n",
    "def QASM_generator(circuitname, max_qubit):\n",
    "    # 创建目录\n",
    "    directory = \"Circuits\"\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    \n",
    "    circuit_func = None\n",
    "    if circuitname == \"grover\":\n",
    "        circuit_func = grover\n",
    "    elif circuitname == \"qft\":\n",
    "        circuit_func = qft\n",
    "    elif circuitname == \"qpe\":\n",
    "        circuit_func = qpe\n",
    "    elif circuitname == \"h_c\":\n",
    "        circuit_func = h_c\n",
    "    elif circuitname == \"rx_c\":\n",
    "        circuit_func = rx_c\n",
    "    elif circuitname == \"rx_gradually_c\":\n",
    "        circuit_func = rx_gradually_c  \n",
    "    else:\n",
    "        print(\"Unsupported circuit name.\")\n",
    "        return\n",
    "\n",
    "    for n in range(2, max_qubit + 1):\n",
    "      \n",
    "        circuit = circuit_func(n)\n",
    "       \n",
    "        qasm_str = circuit.qasm()\n",
    "\n",
    "        filename = os.path.join(directory, f\"{circuitname}_{n}.qasm\")\n",
    "        with open(filename, \"w\") as file:\n",
    "            file.write(qasm_str)\n",
    "        print(f\"Saved {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic idea of genetic programming is apply operations which similar to a natural genetic process to a certain task. During each time of a replication(or so-called off-spring), we randomly select some parameters from the parameter space and do some measurement on our selection. The evaluation of these observation is task-dependent. We select some of with the higher perforamnce and keep them to the next generation. we repeat such iteration turn by turns, keeps the highest or some of the highest scores species(the entity of the selection of the parameter sapce) to the  latter off-spring. in the context of circuit decomplier. For a certain quantum circuit $C_A^N$ (A, correspond to the underlying algorithm and N correspond the the scale of the quabit), we want to learn the underlying pattern for such quantum circuit or in other words, let our circyit Decomplier to \"explain\" the quantum circuit. So the evaluation of each iteration in our genetic programing is how close the circit generated by our decomplier to the ground truth, the actual circuit of the $C_A^N$.\n",
    "\n",
    "Therefore, to implement the Decomplier using Genetic algorthim, we first just start from the simplest case. We regard every circuit $C_A^N$ can be composed of some combination of the \"print\" operation and the \"For\" loop\n",
    "- For the print operation, the parameter space is composed with the typical syntax in QASM file, it include \"Gate Definiation\" \"register\" and \"gate operators\", to begin from the bottle, we start with[h,q ]\n",
    "\n",
    "- For the for loop, we need to determine the number of loop we need, we can can struct it as \n",
    "```python\n",
    "for i in range (1,n,1):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from qiskit import QuantumCircuit\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "class genetic_Decompiler:\n",
    "    def __init__(self, algorithm_name, qubit_limit=20, generations=100, pop_size=50, max_length=10, perform_crossover=True, perform_mutation=True, selection_size=1):\n",
    "        self.algorithm_name = algorithm_name\n",
    "        self.qubit_limit = qubit_limit\n",
    "        self.generations = generations\n",
    "        self.pop_size = pop_size\n",
    "        self.max_length = max_length\n",
    "        self.perform_crossover = perform_crossover\n",
    "        self.perform_mutation = perform_mutation\n",
    "        self.selection_size = selection_size\n",
    "        self.gate_operations = ['h', 'rx']  # Presumed set of quantum operations\n",
    "\n",
    "    def generate_target_circuit_list(self):\n",
    "        circuit_list = []\n",
    "        base_path = \"Circuits\"\n",
    "        for qubit_number in range(2, 41):\n",
    "            file_name = f\"{self.algorithm_name}_{qubit_number}.qasm\"\n",
    "            file_path = os.path.join(base_path, file_name)\n",
    "            circuit = QuantumCircuit.from_qasm_file(file_path)\n",
    "            circuit_list.append(circuit)\n",
    "        return circuit_list\n",
    "\n",
    "    def evaluate_decompilation(self, generated_circuit_code, target_circuit, n):\n",
    "        difference = random.uniform(0, 1)  # Simulated difference score\n",
    "        return difference\n",
    "\n",
    "    def generate_initial_population(self):\n",
    "        population = []\n",
    "        for _ in range(self.pop_size):\n",
    "            circuit_length = random.randint(1, self.max_length)\n",
    "            circuit_code = {'operations': [], 'n': random.randint(2, self.qubit_limit)}\n",
    "            for i in range(circuit_length):\n",
    "                operation = random.choice(self.gate_operations)\n",
    "                qubit_index = f\"q[{random.randint(0, circuit_length-1)}]\"\n",
    "                operation_string = f\"{operation} {qubit_index};\\n\"\n",
    "                if operation == 'rx':\n",
    "                    phase_n = random.randint(3, self.qubit_limit)\n",
    "                    phase = f\"(pi/2^{phase_n})\"\n",
    "                    operation_string = f\"{operation}({phase}) {qubit_index};\\n\"\n",
    "                circuit_code['operations'].append(operation_string)\n",
    "            population.append(circuit_code)\n",
    "        return population\n",
    "\n",
    "    def mutate(self, circuit_code, mutation_rate=0.1):\n",
    "        if random.random() < mutation_rate:\n",
    "            circuit_length = len(circuit_code['operations'])\n",
    "            qubit_index = f\"q[{random.randint(0, circuit_length-1)}]\"\n",
    "            operation = random.choice(self.gate_operations)\n",
    "            operation_string = f\"{operation} {qubit_index};\\n\"\n",
    "            if operation == 'rx':\n",
    "                phase_n = random.randint(3, self.qubit_limit)\n",
    "                phase = f\"(pi/2^{phase_n})\"\n",
    "                operation_string = f\"{operation}({phase}) {qubit_index};\\n\"\n",
    "            circuit_code['operations'].append(operation_string)\n",
    "        return circuit_code\n",
    "\n",
    "    def crossover(self, circuit1, circuit2):\n",
    "        crossover_point = len(circuit1['operations']) // 2\n",
    "        new_operations = circuit1['operations'][:crossover_point] + circuit2['operations'][crossover_point:]\n",
    "        return {'operations': new_operations, 'n': random.choice([circuit1['n'], circuit2['n']])}\n",
    "\n",
    "    def run(self):\n",
    "        target_circuit_list = self.generate_target_circuit_list()\n",
    "        population = self.generate_initial_population()\n",
    "        for generation in range(self.generations):\n",
    "            start_time = time.time()\n",
    "            fitness_scores = []\n",
    "            for circuit_code in population:\n",
    "                total_difference = 0\n",
    "                for target_circuit in target_circuit_list:\n",
    "                    total_difference += self.evaluate_decompilation(circuit_code, target_circuit, circuit_code['n'])\n",
    "                average_difference = total_difference / len(target_circuit_list)\n",
    "                fitness_scores.append(1 / (1 + average_difference))\n",
    "\n",
    "            sorted_population = [x for _, x in sorted(zip(fitness_scores, population), key=lambda pair: pair[0], reverse=True)]\n",
    "            next_generation = sorted_population[:self.selection_size]\n",
    "\n",
    "            offspring = []\n",
    "            while len(offspring) < self.pop_size - len(next_generation):\n",
    "                if self.perform_crossover:\n",
    "                    parent1, parent2 = random.sample(next_generation, 2)\n",
    "                    child = self.crossover(parent1, parent2)\n",
    "                else:\n",
    "                    child = random.choice(next_generation)\n",
    "                if self.perform_mutation:\n",
    "                    child = self.mutate(child)\n",
    "                offspring.append(child)\n",
    "\n",
    "            population = next_generation + offspring\n",
    "            end_time = time.time()\n",
    "            time_taken = end_time - start_time\n",
    "            print(f\"Generation {generation + 1}/{self.generations} completed in {time_taken:.2f} seconds\")\n",
    "\n",
    "        final_scores = []\n",
    "        for circuit_code in population:\n",
    "            total_difference = 0\n",
    "            for target_circuit in target_circuit_list:\n",
    "                total_difference += self.evaluate_decompilation(circuit_code, target_circuit, circuit_code['n'])\n",
    "            average_difference = total_difference / len(target_circuit_list)\n",
    "            final_scores.append(1 / (1 + average_difference))\n",
    "\n",
    "        best_code = population[np.argmax(final_scores)]\n",
    "        return best_code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1/100 completed in 0.00 seconds\n",
      "Generation 2/100 completed in 0.00 seconds\n",
      "Generation 3/100 completed in 0.00 seconds\n",
      "Generation 4/100 completed in 0.00 seconds\n",
      "Generation 5/100 completed in 0.00 seconds\n",
      "Generation 6/100 completed in 0.00 seconds\n",
      "Generation 7/100 completed in 0.00 seconds\n",
      "Generation 8/100 completed in 0.00 seconds\n",
      "Generation 9/100 completed in 0.00 seconds\n",
      "Generation 10/100 completed in 0.00 seconds\n",
      "Generation 11/100 completed in 0.00 seconds\n",
      "Generation 12/100 completed in 0.01 seconds\n",
      "Generation 13/100 completed in 0.00 seconds\n",
      "Generation 14/100 completed in 0.00 seconds\n",
      "Generation 15/100 completed in 0.00 seconds\n",
      "Generation 16/100 completed in 0.00 seconds\n",
      "Generation 17/100 completed in 0.00 seconds\n",
      "Generation 18/100 completed in 0.00 seconds\n",
      "Generation 19/100 completed in 0.00 seconds\n",
      "Generation 20/100 completed in 0.00 seconds\n",
      "Generation 21/100 completed in 0.00 seconds\n",
      "Generation 22/100 completed in 0.00 seconds\n",
      "Generation 23/100 completed in 0.00 seconds\n",
      "Generation 24/100 completed in 0.00 seconds\n",
      "Generation 25/100 completed in 0.00 seconds\n",
      "Generation 26/100 completed in 0.00 seconds\n",
      "Generation 27/100 completed in 0.00 seconds\n",
      "Generation 28/100 completed in 0.00 seconds\n",
      "Generation 29/100 completed in 0.00 seconds\n",
      "Generation 30/100 completed in 0.00 seconds\n",
      "Generation 31/100 completed in 0.00 seconds\n",
      "Generation 32/100 completed in 0.00 seconds\n",
      "Generation 33/100 completed in 0.00 seconds\n",
      "Generation 34/100 completed in 0.00 seconds\n",
      "Generation 35/100 completed in 0.00 seconds\n",
      "Generation 36/100 completed in 0.00 seconds\n",
      "Generation 37/100 completed in 0.00 seconds\n",
      "Generation 38/100 completed in 0.00 seconds\n",
      "Generation 39/100 completed in 0.01 seconds\n",
      "Generation 40/100 completed in 0.00 seconds\n",
      "Generation 41/100 completed in 0.00 seconds\n",
      "Generation 42/100 completed in 0.00 seconds\n",
      "Generation 43/100 completed in 0.00 seconds\n",
      "Generation 44/100 completed in 0.00 seconds\n",
      "Generation 45/100 completed in 0.00 seconds\n",
      "Generation 46/100 completed in 0.00 seconds\n",
      "Generation 47/100 completed in 0.00 seconds\n",
      "Generation 48/100 completed in 0.00 seconds\n",
      "Generation 49/100 completed in 0.00 seconds\n",
      "Generation 50/100 completed in 0.01 seconds\n",
      "Generation 51/100 completed in 0.00 seconds\n",
      "Generation 52/100 completed in 0.00 seconds\n",
      "Generation 53/100 completed in 0.00 seconds\n",
      "Generation 54/100 completed in 0.00 seconds\n",
      "Generation 55/100 completed in 0.00 seconds\n",
      "Generation 56/100 completed in 0.00 seconds\n",
      "Generation 57/100 completed in 0.00 seconds\n",
      "Generation 58/100 completed in 0.01 seconds\n",
      "Generation 59/100 completed in 0.00 seconds\n",
      "Generation 60/100 completed in 0.00 seconds\n",
      "Generation 61/100 completed in 0.00 seconds\n",
      "Generation 62/100 completed in 0.00 seconds\n",
      "Generation 63/100 completed in 0.00 seconds\n",
      "Generation 64/100 completed in 0.01 seconds\n",
      "Generation 65/100 completed in 0.00 seconds\n",
      "Generation 66/100 completed in 0.00 seconds\n",
      "Generation 67/100 completed in 0.00 seconds\n",
      "Generation 68/100 completed in 0.00 seconds\n",
      "Generation 69/100 completed in 0.00 seconds\n",
      "Generation 70/100 completed in 0.00 seconds\n",
      "Generation 71/100 completed in 0.00 seconds\n",
      "Generation 72/100 completed in 0.00 seconds\n",
      "Generation 73/100 completed in 0.02 seconds\n",
      "Generation 74/100 completed in 0.00 seconds\n",
      "Generation 75/100 completed in 0.00 seconds\n",
      "Generation 76/100 completed in 0.00 seconds\n",
      "Generation 77/100 completed in 0.00 seconds\n",
      "Generation 78/100 completed in 0.00 seconds\n",
      "Generation 79/100 completed in 0.00 seconds\n",
      "Generation 80/100 completed in 0.01 seconds\n",
      "Generation 81/100 completed in 0.00 seconds\n",
      "Generation 82/100 completed in 0.00 seconds\n",
      "Generation 83/100 completed in 0.00 seconds\n",
      "Generation 84/100 completed in 0.00 seconds\n",
      "Generation 85/100 completed in 0.00 seconds\n",
      "Generation 86/100 completed in 0.00 seconds\n",
      "Generation 87/100 completed in 0.00 seconds\n",
      "Generation 88/100 completed in 0.01 seconds\n",
      "Generation 89/100 completed in 0.00 seconds\n",
      "Generation 90/100 completed in 0.00 seconds\n",
      "Generation 91/100 completed in 0.00 seconds\n",
      "Generation 92/100 completed in 0.00 seconds\n",
      "Generation 93/100 completed in 0.00 seconds\n",
      "Generation 94/100 completed in 0.00 seconds\n",
      "Generation 95/100 completed in 0.00 seconds\n",
      "Generation 96/100 completed in 0.00 seconds\n",
      "Generation 97/100 completed in 0.00 seconds\n",
      "Generation 98/100 completed in 0.00 seconds\n",
      "Generation 99/100 completed in 0.00 seconds\n",
      "Generation 100/100 completed in 0.00 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'operations': ['h q[6];\\n',\n",
       "  'rx((pi/2^7)) q[5];\\n',\n",
       "  'rx((pi/2^14)) q[2];\\n',\n",
       "  'rx((pi/2^5)) q[5];\\n',\n",
       "  'rx((pi/2^9)) q[0];\\n',\n",
       "  'rx((pi/2^8)) q[2];\\n',\n",
       "  'h q[4];\\n',\n",
       "  'h q[0];\\n'],\n",
       " 'n': 15}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Decompiler=genetic_Decompiler(algorithm_name='h_c',qubit_limit=20,perform_crossover=False,perform_mutation=False)\n",
    "Decompiler.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Atempt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operator([[0.01104854+0.00000000e+00j, 0.01104854-1.35305634e-18j,\n",
      "           0.01104854-1.35305634e-18j, ..., 0.01104854-1.62366761e-17j,\n",
      "           0.01104854-1.62366761e-17j, 0.01104854-1.75897324e-17j],\n",
      "          [0.01104854+0.00000000e+00j, 0.01104854+8.47412587e-06j,\n",
      "           0.01104853+1.69482467e-05j, ..., 0.01104851-2.54223577e-05j,\n",
      "           0.01104853-1.69482467e-05j, 0.01104854-8.47412587e-06j],\n",
      "          [0.01104854+0.00000000e+00j, 0.01104853+1.69482467e-05j,\n",
      "           0.01104849+3.38964536e-05j, ..., 0.01104843-5.08445807e-05j,\n",
      "           0.01104849-3.38964536e-05j, 0.01104853-1.69482467e-05j],\n",
      "          ...,\n",
      "          [0.01104854+0.00000000e+00j, 0.01104851-2.54223577e-05j,\n",
      "           0.01104843-5.08445807e-05j, ..., 0.01104828+7.62665346e-05j,\n",
      "           0.01104843+5.08445807e-05j, 0.01104851+2.54223577e-05j],\n",
      "          [0.01104854+0.00000000e+00j, 0.01104853-1.69482467e-05j,\n",
      "           0.01104849-3.38964536e-05j, ..., 0.01104843+5.08445807e-05j,\n",
      "           0.01104849+3.38964536e-05j, 0.01104853+1.69482467e-05j],\n",
      "          [0.01104854+0.00000000e+00j, 0.01104854-8.47412587e-06j,\n",
      "           0.01104853-1.69482467e-05j, ..., 0.01104851+2.54223577e-05j,\n",
      "           0.01104853+1.69482467e-05j, 0.01104854+8.47412587e-06j]],\n",
      "         input_dims=(2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2), output_dims=(2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2))\n"
     ]
    }
   ],
   "source": [
    "from qiskit import QuantumCircuit, Aer, transpile\n",
    "from qiskit.quantum_info import Operator\n",
    "\n",
    "from qiskit import QuantumCircuit\n",
    "\n",
    "def remove_non_unitary_operations(original_circuit):\n",
    "    # Create a new quantum circuit with the same number of qubits and classical bits\n",
    "    new_circuit = QuantumCircuit(original_circuit.num_qubits)\n",
    "    \n",
    "    # Iterate over all operations in the original circuit\n",
    "    for instr, qargs, cargs in original_circuit.data:\n",
    "        # Check if the operation is unitary (gates are considered unitary)\n",
    "        if instr.name not in ['measure', 'reset']:\n",
    "            # If the operation is unitary, add it to the new circuit\n",
    "            new_circuit.append(instr, qargs, cargs)\n",
    "    \n",
    "    return new_circuit\n",
    "\n",
    "\n",
    "def qasm_to_unitary(circuit):\n",
    "    \n",
    "    # Use the Aer's unitary simulator\n",
    "    simulator = Aer.get_backend('unitary_simulator')\n",
    "    \n",
    "    # Transpile the circuit for the simulator\n",
    "    transpiled_circuit = transpile(circuit, simulator)\n",
    "    \n",
    "    # Run the simulation to get the unitary\n",
    "    job = simulator.run(transpiled_circuit)\n",
    "    result = job.result()\n",
    "    \n",
    "    # Get the unitary matrix from the result\n",
    "    unitary = result.get_unitary(transpiled_circuit)\n",
    "    \n",
    "    return unitary\n",
    "\n",
    "# Example usage\n",
    "qasm_path = 'D:/course/thesis/Project2/QCD/Circuits/qft_13.qasm'\n",
    "original_circuit = QuantumCircuit.from_qasm_file(qasm_path)\n",
    "unitary_circuit = remove_non_unitary_operations(original_circuit)\n",
    "unitary_matrix = qasm_to_unitary(unitary_circuit)\n",
    "print(unitary_matrix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Circuit 0:\n",
      "\n",
      "     ┌───┐     ┌─┐   \n",
      "q_0: ┤ H ├──■──┤M├───\n",
      "     └───┘┌─┴─┐└╥┘┌─┐\n",
      "q_1: ─────┤ X ├─╫─┤M├\n",
      "      ┌─┐ └───┘ ║ └╥┘\n",
      "q_2: ─┤M├───────╫──╫─\n",
      "      └╥┘       ║  ║ \n",
      "c: 3/══╩════════╩══╩═\n",
      "       2        0  1 \n",
      "Circuit 1:\n",
      "\n",
      "     ┌───┐     ┌─┐   \n",
      "q_0: ┤ H ├──■──┤M├───\n",
      "     └───┘┌─┴─┐└╥┘┌─┐\n",
      "q_1: ─────┤ X ├─╫─┤M├\n",
      "      ┌─┐ └───┘ ║ └╥┘\n",
      "q_2: ─┤M├───────╫──╫─\n",
      "      └╥┘       ║  ║ \n",
      "c: 3/══╩════════╩══╩═\n",
      "       2        0  1 \n",
      "Circuit 2:\n",
      "\n",
      "     ┌───┐     ┌─┐   \n",
      "q_0: ┤ H ├──■──┤M├───\n",
      "     └───┘┌─┴─┐└╥┘┌─┐\n",
      "q_1: ─────┤ X ├─╫─┤M├\n",
      "      ┌─┐ └───┘ ║ └╥┘\n",
      "q_2: ─┤M├───────╫──╫─\n",
      "      └╥┘       ║  ║ \n",
      "c: 3/══╩════════╩══╩═\n",
      "       2        0  1 \n"
     ]
    }
   ],
   "source": [
    "from qiskit import QuantumCircuit\n",
    "\n",
    "# 创建一个空列表来存放Qasm量子电路\n",
    "quantum_circuits = []\n",
    "\n",
    "# 创建几个简单的量子电路作为示例\n",
    "for i in range(3):\n",
    "    # 创建一个包含3个量子比特和3个经典比特的量子电路\n",
    "    qc = QuantumCircuit(3, 3)\n",
    "    \n",
    "    # 添加一些量子门操作作为示例\n",
    "    qc.h(0)  # 将Hadamard门应用于第一个量子比特\n",
    "    qc.cx(0, 1)  # 添加一个CNOT门，控制量子比特为0，目标量子比特为1\n",
    "    qc.measure([0, 1, 2], [0, 1, 2])  # 测量所有量子比特，并将结果存储到所有经典比特\n",
    "    \n",
    "    # 将创建的量子电路添加到列表中\n",
    "    quantum_circuits.append(qc)\n",
    "\n",
    "# 打印出列表中的每一个量子电路来验证\n",
    "for i, qc in enumerate(quantum_circuits):\n",
    "    print(f\"Circuit {i}:\\n\")\n",
    "    print(qc.draw())  # 使用.draw()方法来可视化电路\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
